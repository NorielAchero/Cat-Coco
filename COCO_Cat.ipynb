{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing COCO Yolo Model in Roboflow by detecting Cats Images with Segmentation Techniques"
      ],
      "metadata": {
        "id": "sb3w2CE_ELI7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS0KcCweECCd",
        "outputId": "660d5be3-25e4-4ea6-cd9d-0b8856123405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: inference-sdk in /usr/local/lib/python3.10/dist-packages (0.31.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: dataclasses-json~=0.6.0 in /usr/local/lib/python3.10/dist-packages (from inference-sdk) (0.6.7)\n",
            "Requirement already satisfied: opencv-python<=4.10.0.84,>=4.8.1.78 in /usr/local/lib/python3.10/dist-packages (from inference-sdk) (4.10.0.84)\n",
            "Requirement already satisfied: supervision<=0.30.0,>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from inference-sdk) (0.25.1)\n",
            "Requirement already satisfied: aiohttp<=3.10.11,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from inference-sdk) (3.10.11)\n",
            "Requirement already satisfied: backoff~=2.2.0 in /usr/local/lib/python3.10/dist-packages (from inference-sdk) (2.2.1)\n",
            "Requirement already satisfied: py-cpuinfo~=9.0.0 in /usr/local/lib/python3.10/dist-packages (from inference-sdk) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (1.18.3)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json~=0.6.0->inference-sdk) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json~=0.6.0->inference-sdk) (0.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision<=0.30.0,>=0.25.1->inference-sdk) (0.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from supervision<=0.30.0,>=0.25.1->inference-sdk) (6.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision<=0.30.0,>=0.25.1->inference-sdk) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from supervision<=0.30.0,>=0.25.1->inference-sdk) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp<=3.10.11,>=3.9.0->inference-sdk) (4.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json~=0.6.0->inference-sdk) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<=3.10.11,>=3.9.0->inference-sdk) (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "pip install requests matplotlib pillow inference-sdk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from io import BytesIO\n",
        "import json\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "\n",
        "# Initialize the Roboflow Inference Client\n",
        "CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"IJgd8jCLHIdJE3mNjMvJ\"\n",
        ")\n",
        "\n",
        "def display_image_with_boxes(image_path, detections):\n",
        "    \"\"\"\n",
        "    Display an image with bounding boxes overlaid.\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        detections (list): List of detection results containing bounding box coordinates.\n",
        "    \"\"\"\n",
        "    # Open the image\n",
        "    img = Image.open(image_path)\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Draw each bounding box\n",
        "    for detection in detections:\n",
        "        x, y, w, h = (\n",
        "            detection[\"x\"],\n",
        "            detection[\"y\"],\n",
        "            detection[\"width\"],\n",
        "            detection[\"height\"],\n",
        "        )\n",
        "        label = detection[\"class\"]\n",
        "        confidence = detection[\"confidence\"]\n",
        "\n",
        "        # Calculate bounding box coordinates\n",
        "        left = x - (w / 2)\n",
        "        top = y - (h / 2)\n",
        "\n",
        "        # Create a rectangle patch\n",
        "        rect = patches.Rectangle(\n",
        "            (left, top), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(left, top, f\"{label}: {confidence:.2f}\", color=\"yellow\", fontsize=8)\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Path to the image to be detected\n",
        "    image_path = \"sample.jpg\"  # Replace with the path to your image file\n",
        "\n",
        "    # Run inference using Roboflow\n",
        "    try:\n",
        "        print(\"Running inference...\")\n",
        "        result = CLIENT.infer(image_path, model_id=\"coco-0qrql/1\")\n",
        "\n",
        "        # Parse detection results\n",
        "        detections = result[\"predictions\"]\n",
        "        print(\"Detections:\", json.dumps(detections, indent=2))\n",
        "\n",
        "        # Display the image with bounding boxes\n",
        "        display_image_with_boxes(image_path, detections)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error during inference:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaeWORqMFK8H",
        "outputId": "990839a5-e1b3-478f-b139-4b94bc5c09c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference...\n",
            "Error during inference: HTTPCallErrorError(description='400 Client Error: Bad Request for url: https://detect.roboflow.com/coco-0qrql/1?api_key=IJ***vJ&disable_active_learning=False', api_message='Could not load input image. Cause: Malformed base64 input image.',status_code=400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image in color (BGR format)\n",
        "img = cv2.imread(r'/content/sample.jpg')\n",
        "\n",
        "# Convert the BGR image to grayscale for Otsu's thresholding\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Otsu's thresholding\n",
        "ret, thresh = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "# Display the original (color) and thresholded (grayscale) images using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original color image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
        "plt.title('Original Color Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Otsu Thresholding result\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(thresh, cmap='gray')\n",
        "plt.title(\"Otsu's Thresholding (Grayscale)\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Show the images\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "fD6Z3I65Fb0y",
        "outputId": "bb74256d-7d82-4193-edd8-6f31ccf65af5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3cf083f7d0f4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert the BGR image to grayscale for Otsu's thresholding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgray_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Apply Otsu's thresholding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image in color\n",
        "img = cv2.imread(r'/content/sample.jpg')\n",
        "\n",
        "# Convert to grayscale for adaptive thresholding\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply mean adaptive thresholding\n",
        "thresh1 = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "# Apply Gaussian adaptive thresholding\n",
        "thresh2 = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "# Display the original and thresholded images using matplotlib\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original image (color)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Mean Adaptive Thresholding result\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(thresh1, cmap='gray')\n",
        "plt.title('Mean Adaptive Thresholding')\n",
        "plt.axis('off')\n",
        "\n",
        "# Gaussian Adaptive Thresholding result\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(thresh2, cmap='gray')\n",
        "plt.title('Gaussian Adaptive Thresholding')\n",
        "plt.axis('off')\n",
        "\n",
        "# Show the images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S8Yi5GZFF2tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def region_growing(image, seed_point, threshold):\n",
        "    \"\"\"\n",
        "    Performs region growing on an image.\n",
        "\n",
        "    Args:\n",
        "        image: The input image (grayscale).\n",
        "        seed_point: The coordinates of the seed point.\n",
        "        threshold: The similarity threshold.\n",
        "\n",
        "    Returns:\n",
        "        A binary mask representing the segmented region.\n",
        "    \"\"\"\n",
        "    rows, cols = image.shape\n",
        "    mask = np.zeros_like(image)\n",
        "    queue = [seed_point]\n",
        "\n",
        "    while queue:\n",
        "        x, y = queue.pop(0)\n",
        "        if mask[x, y] == 0:\n",
        "            mask[x, y] = 1\n",
        "            neighbors = [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)]\n",
        "            for nx, ny in neighbors:\n",
        "                if 0 <= nx < rows and 0 <= ny < cols and abs(int(image[x, y]) - int(image[nx, ny])) <= threshold:\n",
        "                    queue.append((nx, ny))\n",
        "\n",
        "    return mask\n",
        "\n",
        "# Example usage\n",
        "image = cv2.imread(r'/content/sample.jpg')  # Load in color\n",
        "seed_point = (100, 100)  # Set the seed point\n",
        "threshold = 10  # Set the similarity threshold\n",
        "\n",
        "# Convert the image to grayscale for region growing\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "mask = region_growing(gray_image, seed_point, threshold)\n",
        "\n",
        "# Display the original color image and region growing result using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original image (in color)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Region Growing result\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.title('Region Growing')\n",
        "plt.axis('off')\n",
        "\n",
        "# Show the images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CiinD-yXF7CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the image in color (BGR format)\n",
        "image = cv2.imread(r'/content/sample.jpg')\n",
        "\n",
        "# Reshape the image to a 2D array of pixels (3 channels)\n",
        "img_reshaped = image.reshape((-1, 3))\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(img_reshaped)\n",
        "\n",
        "# Get the labels and cluster centers\n",
        "labels = kmeans.labels_\n",
        "centers = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "# Reshape the labels back to the original image shape\n",
        "labels = labels.reshape(image.shape[:2])\n",
        "\n",
        "# Create the segmented image using the cluster centers\n",
        "segmented_image = np.zeros_like(image)\n",
        "for i in range(len(centers)):\n",
        "    segmented_image[labels == i] = centers[i]\n",
        "\n",
        "# Display the original and segmented images using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Segmented image\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
        "plt.title('K-Means Clustering')\n",
        "plt.axis('off')\n",
        "\n",
        "# Show the images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nlePmWi4F_NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "# Load the image\n",
        "img = cv2.imread(r'/content/sample.jpg')\n",
        "\n",
        "# Resize the image to a smaller size (for example, to 100x100)\n",
        "img_resized = cv2.resize(img, (100, 100))\n",
        "\n",
        "# Reshape the resized image to a 2D array (each pixel is a point)\n",
        "img_reshaped = img_resized.reshape((-1, 3))\n",
        "\n",
        "# Perform mean shift clustering\n",
        "ms = MeanShift()\n",
        "ms.fit(img_reshaped)\n",
        "labels = ms.labels_\n",
        "centers = ms.cluster_centers_\n",
        "\n",
        "# Reshape the labels back to the 2D image shape\n",
        "labels = labels.reshape(img_resized.shape[:2])\n",
        "\n",
        "# Create the segmented image by assigning the cluster centers to each pixel\n",
        "segmented_image = np.zeros_like(img_resized)\n",
        "for i in range(len(centers)):\n",
        "    segmented_image[labels == i] = centers[i]\n",
        "\n",
        "# Display the original (resized) and segmented images using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Resized original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
        "plt.title('Original Resized Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Mean Shift Clustering result\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for matplotlib\n",
        "plt.title(\"Mean Shift Clustering\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Show the images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nhhQRMBpGMvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OTSU - Cat Detect"
      ],
      "metadata": {
        "id": "uyNIBIfDHmLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the Roboflow Client\n",
        "CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"IJgd8jCLHIdJE3mNjMvJ\"\n",
        ")\n",
        "\n",
        "def save_temp_image(image, path=\"temp_thresh_image.jpg\"):\n",
        "    \"\"\"\n",
        "    Save the thresholded image temporarily for detection.\n",
        "    Args:\n",
        "        image (numpy array): Otsu thresholded image.\n",
        "        path (str): Path to save the image.\n",
        "    \"\"\"\n",
        "    cv2.imwrite(path, image)\n",
        "    return path\n",
        "\n",
        "def run_inference(image_path):\n",
        "    \"\"\"Run object detection using Roboflow API and return detections.\"\"\"\n",
        "    result = CLIENT.infer(image_path, model_id=\"coco-0qrql/1\")  # Replace model ID if needed\n",
        "    return result[\"predictions\"]\n",
        "\n",
        "def display_image_with_boxes(image_path, detections):\n",
        "    \"\"\"Display the thresholded image with bounding boxes from detections.\"\"\"\n",
        "    # Open the image using PIL\n",
        "    img = Image.open(image_path).convert(\"RGB\")  # Ensure it's RGB for visualization\n",
        "    fig, ax = plt.subplots(1, figsize=(10, 6))\n",
        "    ax.imshow(img, cmap=\"gray\")\n",
        "\n",
        "    # Add bounding boxes\n",
        "    for detection in detections:\n",
        "        x, y, w, h = detection[\"x\"], detection[\"y\"], detection[\"width\"], detection[\"height\"]\n",
        "        label = detection[\"class\"]\n",
        "        confidence = detection[\"confidence\"]\n",
        "\n",
        "        # Convert to bounding box (left, top)\n",
        "        left = x - w / 2\n",
        "        top = y - h / 2\n",
        "\n",
        "        # Draw bounding box\n",
        "        rect = patches.Rectangle((left, top), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(left, top - 5, f\"{label}: {confidence:.2f}\", color=\"yellow\", fontsize=8)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Object Detection on Otsu Thresholded Image\")\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Path to the original image\n",
        "    image_path = r'/content/sample.jpg'\n",
        "\n",
        "    # Step 1: Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Step 2: Convert the image to grayscale\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Step 3: Apply Otsu's thresholding\n",
        "    ret, thresh = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Step 4: Save the Otsu thresholded image temporarily\n",
        "    thresh_image_path = save_temp_image(thresh)\n",
        "\n",
        "    # Step 5: Run object detection using Roboflow API\n",
        "    print(\"Running object detection on Otsu thresholded image...\")\n",
        "    detections = run_inference(thresh_image_path)\n",
        "    print(\"Detections:\", detections)\n",
        "\n",
        "    # Step 6: Display the Otsu thresholded image with bounding boxes\n",
        "    display_image_with_boxes(thresh_image_path, detections)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "hs_wBCC7GR7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptive Thresholdings"
      ],
      "metadata": {
        "id": "DviXGJXyIYqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the Roboflow Client\n",
        "CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"IJgd8jCLHIdJE3mNjMvJ\"\n",
        ")\n",
        "\n",
        "def save_temp_image(image, path):\n",
        "    \"\"\"\n",
        "    Save a thresholded image temporarily for detection.\n",
        "    Args:\n",
        "        image (numpy array): Thresholded image.\n",
        "        path (str): Path to save the image.\n",
        "    \"\"\"\n",
        "    cv2.imwrite(path, image)\n",
        "    return path\n",
        "\n",
        "def run_inference(image_path):\n",
        "    \"\"\"Run object detection using Roboflow API and return detections.\"\"\"\n",
        "    result = CLIENT.infer(image_path, model_id=\"coco-0qrql/1\")\n",
        "    return result[\"predictions\"]\n",
        "\n",
        "def display_image_with_boxes(image_path, detections, title):\n",
        "    \"\"\"Display a thresholded image with bounding boxes from detections.\"\"\"\n",
        "    # Open the image using PIL\n",
        "    img = Image.open(image_path)\n",
        "    fig, ax = plt.subplots(1, figsize=(7, 5))\n",
        "    ax.imshow(img, cmap=\"gray\")\n",
        "\n",
        "    # Add bounding boxes\n",
        "    for detection in detections:\n",
        "        x, y, w, h = detection[\"x\"], detection[\"y\"], detection[\"width\"], detection[\"height\"]\n",
        "        label = detection[\"class\"]\n",
        "        confidence = detection[\"confidence\"]\n",
        "\n",
        "        # Convert to bounding box (left, top)\n",
        "        left = x - w / 2\n",
        "        top = y - h / 2\n",
        "\n",
        "        # Draw bounding box\n",
        "        rect = patches.Rectangle((left, top), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(left, top - 5, f\"{label}: {confidence:.2f}\", color=\"yellow\", fontsize=8)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Path to the input image\n",
        "    image_path = r'/content/sample.jpg'\n",
        "\n",
        "    # Step 1: Load and process the image\n",
        "    img = cv2.imread(image_path)\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Step 2: Apply adaptive thresholding\n",
        "    mean_thresh = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    gaussian_thresh = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "    # Step 3: Save thresholded images temporarily\n",
        "    mean_thresh_path = save_temp_image(mean_thresh, \"mean_thresh.jpg\")\n",
        "    gaussian_thresh_path = save_temp_image(gaussian_thresh, \"gaussian_thresh.jpg\")\n",
        "\n",
        "    # Step 4: Run object detection\n",
        "    print(\"Running object detection on Mean Adaptive Thresholding...\")\n",
        "    mean_detections = run_inference(mean_thresh_path)\n",
        "    print(\"Detections (Mean Thresholding):\", mean_detections)\n",
        "\n",
        "    print(\"Running object detection on Gaussian Adaptive Thresholding...\")\n",
        "    gaussian_detections = run_inference(gaussian_thresh_path)\n",
        "    print(\"Detections (Gaussian Thresholding):\", gaussian_detections)\n",
        "\n",
        "    # Step 5: Display thresholded images with detection results\n",
        "    display_image_with_boxes(mean_thresh_path, mean_detections, \"Mean Adaptive Thresholding Detection\")\n",
        "    display_image_with_boxes(gaussian_thresh_path, gaussian_detections, \"Gaussian Adaptive Thresholding Detection\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "anrlW2QxHo0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize the Roboflow Client\n",
        "CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"IJgd8jCLHIdJE3mNjMvJ\"\n",
        ")\n",
        "\n",
        "def region_growing(image, seed_point, threshold):\n",
        "    \"\"\"\n",
        "    Performs region growing on an image.\n",
        "\n",
        "    Args:\n",
        "        image: The input image (grayscale).\n",
        "        seed_point: The coordinates of the seed point.\n",
        "        threshold: The similarity threshold.\n",
        "\n",
        "    Returns:\n",
        "        A binary mask representing the segmented region.\n",
        "    \"\"\"\n",
        "    rows, cols = image.shape\n",
        "    mask = np.zeros_like(image)\n",
        "    queue = [seed_point]\n",
        "\n",
        "    while queue:\n",
        "        x, y = queue.pop(0)\n",
        "        if mask[x, y] == 0:\n",
        "            mask[x, y] = 255  # Mark as segmented\n",
        "            neighbors = [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)]\n",
        "            for nx, ny in neighbors:\n",
        "                if 0 <= nx < rows and 0 <= ny < cols and abs(int(image[x, y]) - int(image[nx, ny])) <= threshold:\n",
        "                    queue.append((nx, ny))\n",
        "\n",
        "    return mask\n",
        "\n",
        "def save_temp_image(image, path=\"temp_region_growing.jpg\"):\n",
        "    \"\"\"\n",
        "    Saves the region-growing result temporarily.\n",
        "    \"\"\"\n",
        "    cv2.imwrite(path, image)\n",
        "    return path\n",
        "\n",
        "def run_inference(image_path):\n",
        "    \"\"\"\n",
        "    Runs inference using Roboflow API.\n",
        "    \"\"\"\n",
        "    result = CLIENT.infer(image_path, model_id=\"coco-0qrql/1\")  # Replace model_id as needed\n",
        "    return result[\"predictions\"]\n",
        "\n",
        "def display_detection(image_path, detections):\n",
        "    \"\"\"\n",
        "    Display the region-growing result with bounding boxes.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    fig, ax = plt.subplots(1, figsize=(7, 5))\n",
        "    ax.imshow(img, cmap=\"gray\")\n",
        "\n",
        "    for detection in detections:\n",
        "        x, y, w, h = detection[\"x\"], detection[\"y\"], detection[\"width\"], detection[\"height\"]\n",
        "        label = detection[\"class\"]\n",
        "        confidence = detection[\"confidence\"]\n",
        "\n",
        "        # Bounding box\n",
        "        left = x - w / 2\n",
        "        top = y - h / 2\n",
        "        rect = patches.Rectangle((left, top), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(left, top - 5, f\"{label}: {confidence:.2f}\", color=\"yellow\", fontsize=8)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Region Growing Detection\")\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Path to the input image\n",
        "    image_path = r'/content/sample.jpg'\n",
        "\n",
        "    # Step 1: Load the image and convert to grayscale\n",
        "    image = cv2.imread(image_path)\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Step 2: Apply Region Growing\n",
        "    seed_point = (100, 100)  # Set a seed point\n",
        "    threshold = 10  # Similarity threshold\n",
        "    print(\"Applying Region Growing...\")\n",
        "    region_mask = region_growing(gray_image, seed_point, threshold)\n",
        "\n",
        "    # Step 3: Save the segmented mask as a temporary file\n",
        "    temp_path = save_temp_image(region_mask)\n",
        "\n",
        "    # Step 4: Run inference on the region-growing result\n",
        "    print(\"Running object detection on Region Growing result...\")\n",
        "    detections = run_inference(temp_path)\n",
        "    print(\"Detections:\", detections)\n",
        "\n",
        "    # Step 5: Display the Region Growing result with detection bounding boxes\n",
        "    display_detection(temp_path, detections)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZHMAbjFnIx9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize the Roboflow Client\n",
        "CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"IJgd8jCLHIdJE3mNjMvJ\"\n",
        ")\n",
        "\n",
        "def save_temp_image(image, path=\"temp_kmeans_segmented.jpg\"):\n",
        "    \"\"\"\n",
        "    Save the segmented image temporarily for detection.\n",
        "    Args:\n",
        "        image (numpy array): Segmented image.\n",
        "        path (str): Path to save the image.\n",
        "    \"\"\"\n",
        "    cv2.imwrite(path, image)\n",
        "    return path\n",
        "\n",
        "def run_inference(image_path):\n",
        "    \"\"\"\n",
        "    Runs inference using Roboflow API.\n",
        "    \"\"\"\n",
        "    result = CLIENT.infer(image_path, model_id=\"coco-0qrql/1\")  # Replace model_id as needed\n",
        "    return result[\"predictions\"]\n",
        "\n",
        "def display_detection(image_path, detections, title=\"K-Means Segmentation Detection\"):\n",
        "    \"\"\"\n",
        "    Display the segmented image with bounding boxes.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    fig, ax = plt.subplots(1, figsize=(7, 5))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    for detection in detections:\n",
        "        x, y, w, h = detection[\"x\"], detection[\"y\"], detection[\"width\"], detection[\"height\"]\n",
        "        label = detection[\"class\"]\n",
        "        confidence = detection[\"confidence\"]\n",
        "\n",
        "        # Bounding box\n",
        "        left = x - w / 2\n",
        "        top = y - h / 2\n",
        "        rect = patches.Rectangle((left, top), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(left, top - 5, f\"{label}: {confidence:.2f}\", color=\"yellow\", fontsize=8)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the image\n",
        "    image_path = r'/content/sample.jpg'\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Step 2: Reshape the image and apply K-means clustering\n",
        "    print(\"Applying K-means clustering...\")\n",
        "    img_reshaped = image.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "    kmeans.fit(img_reshaped)\n",
        "\n",
        "    # Step 3: Get cluster labels and centers\n",
        "    labels = kmeans.labels_.reshape(image.shape[:2])\n",
        "    centers = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "    # Step 4: Create the segmented image\n",
        "    segmented_image = np.zeros_like(image)\n",
        "    for i in range(len(centers)):\n",
        "        segmented_image[labels == i] = centers[i]\n",
        "\n",
        "    # Step 5: Save the segmented image temporarily\n",
        "    segmented_image_path = save_temp_image(segmented_image)\n",
        "\n",
        "    # Step 6: Run inference on the segmented image\n",
        "    print(\"Running object detection on segmented image...\")\n",
        "    detections = run_inference(segmented_image_path)\n",
        "    print(\"Detections:\", detections)\n",
        "\n",
        "    # Step 7: Display the segmented image with detection results\n",
        "    display_detection(segmented_image_path, detections)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Tw6P7xLmLUBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import MeanShift\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize the Roboflow Client\n",
        "CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"IJgd8jCLHIdJE3mNjMvJ\"\n",
        ")\n",
        "\n",
        "def save_temp_image(image, path=\"temp_meanshift_segmented.jpg\"):\n",
        "    \"\"\"\n",
        "    Save the segmented image temporarily for detection.\n",
        "    Args:\n",
        "        image (numpy array): Segmented image.\n",
        "        path (str): Path to save the image.\n",
        "    \"\"\"\n",
        "    cv2.imwrite(path, image)\n",
        "    return path\n",
        "\n",
        "def run_inference(image_path):\n",
        "    \"\"\"\n",
        "    Runs inference using Roboflow API.\n",
        "    \"\"\"\n",
        "    result = CLIENT.infer(image_path, model_id=\"coco-0qrql/1\")  # Replace model_id as needed\n",
        "    return result[\"predictions\"]\n",
        "\n",
        "def display_detection(image_path, detections, title=\"Mean Shift Segmentation Detection\"):\n",
        "    \"\"\"\n",
        "    Display the segmented image with bounding boxes.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    fig, ax = plt.subplots(1, figsize=(7, 5))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    for detection in detections:\n",
        "        x, y, w, h = detection[\"x\"], detection[\"y\"], detection[\"width\"], detection[\"height\"]\n",
        "        label = detection[\"class\"]\n",
        "        confidence = detection[\"confidence\"]\n",
        "\n",
        "        # Bounding box\n",
        "        left = x - w / 2\n",
        "        top = y - h / 2\n",
        "        rect = patches.Rectangle((left, top), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(left, top - 5, f\"{label}: {confidence:.2f}\", color=\"yellow\", fontsize=8)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the image and resize\n",
        "    image_path = r'/content/sample.jpg'\n",
        "    img = cv2.imread(image_path)\n",
        "    img_resized = cv2.resize(img, (100, 100))  # Resize for faster processing\n",
        "\n",
        "    # Step 2: Reshape the image and apply Mean Shift clustering\n",
        "    print(\"Applying Mean Shift clustering...\")\n",
        "    img_reshaped = img_resized.reshape((-1, 3))\n",
        "    ms = MeanShift()\n",
        "    ms.fit(img_reshaped)\n",
        "\n",
        "    # Step 3: Get cluster labels and centers\n",
        "    labels = ms.labels_.reshape(img_resized.shape[:2])\n",
        "    centers = ms.cluster_centers_.astype(np.uint8)\n",
        "\n",
        "    # Step 4: Create the segmented image\n",
        "    segmented_image = np.zeros_like(img_resized)\n",
        "    for i in range(len(centers)):\n",
        "        segmented_image[labels == i] = centers[i]\n",
        "\n",
        "    # Step 5: Save the segmented image temporarily\n",
        "    segmented_image_path = save_temp_image(segmented_image)\n",
        "\n",
        "    # Step 6: Run inference on the segmented image\n",
        "    print(\"Running object detection on segmented image...\")\n",
        "    detections = run_inference(segmented_image_path)\n",
        "    print(\"Detections:\", detections)\n",
        "\n",
        "    # Step 7: Display the segmented image with detection results\n",
        "    display_detection(segmented_image_path, detections)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "JurcMSMrMi19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcF-mM4jOVu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}